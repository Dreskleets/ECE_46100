#Scoring Metrics (With Latency):
#Ramp Up Time
#Bus Factor
#License Compatibility
#Size Score
#Dataset & Code Availability
#Dataset Quality
#Code Quality
#Performance Claims

#Make a file for user to rate the AI after they use it and add it to database.
#Have both the runtime and user ratings stored after exiting the chat.

#Ramp up time from start to go to menu -> select ai -> download ai -> begin chat with ai locally
#Also looking at program startup -> select ai -> begin chat with ai virtually through HF

#Bus Factor is how well could the team recoup if we lost a valuable member of the team who created for example the whole database code
#and suddenly dissapeared / was unable to contribute any information
#Complexity & Documentation of the code base

#License Compatibility is the legality of distrubuting the program with the combined program licenses (SQLite for example)

#Size score is the measurement of size (Lines of Code, Function Points, or Use Case Points, etc.)

#A Data Availability Statement is provided with detailed enough information such that an independent researcher can replicate 
#the steps needed to access the original data, including any limitations and the expected monetary and time cost of data access.

#Dataset Quality is the overall Accuracy, Completeness, Consistency, and Valid Data within a dataset.

#Code quality is the overall Maintanability, readability, reliability, and security of the code base

#Use a net score to validate these metrics into a single metric.